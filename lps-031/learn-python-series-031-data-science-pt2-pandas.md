
# Learn Python Series (#31) - Data Science Part 2 - Pandas

### Repository
- https://github.com/pandas-dev/pandas
- https://github.com/python/cpython

### What will I learn?
- You will learn how to convert date/datetime strings into `pandas` Timestamps, and set those Timestamp values as index values instead of default integer numbers which don't provide useful context;
- how to slice subsets of TimeSeries data using date strings;
- how to slice time-based subsets of TimeSeries data using `.between_time()`;
- how to return and use some basic statistics based on common `pandas` statistical methods.

### Requirements
- A working modern computer running macOS, Windows or Ubuntu;
- An installed Python 3(.7) distribution, such as (for example) the Anaconda Distribution;
- The ambition to learn Python programming.

### Difficulty
- Beginner

### Curriculum (of the `Learn Python Series`):

- [Learn Python Series - Intro](https://steemit.com/utopian-io/@scipio/learn-python-series-intro)
- [Learn Python Series (#2) - Handling Strings Part 1](https://steemit.com/utopian-io/@scipio/learn-python-series-2-handling-strings-part-1)
- [Learn Python Series (#3) - Handling Strings Part 2](https://steemit.com/utopian-io/@scipio/learn-python-series-3-handling-strings-part-2)
- [Learn Python Series (#4) - Round-Up #1](https://steemit.com/utopian-io/@scipio/learn-python-series-4-round-up-1)
- [Learn Python Series (#5) - Handling Lists Part 1](https://steemit.com/utopian-io/@scipio/learn-python-series-5-handling-lists-part-1)
- [Learn Python Series (#6) - Handling Lists Part 2](https://steemit.com/utopian-io/@scipio/learn-python-series-6-handling-lists-part-2)
- [Learn Python Series (#7) - Handling Dictionaries](https://steemit.com/utopian-io/@scipio/learn-python-series-7-handling-dictionaries)
- [Learn Python Series (#8) - Handling Tuples](https://steemit.com/utopian-io/@scipio/learn-python-series-8-handling-tuples)
- [Learn Python Series (#9) - Using Import](https://steemit.com/utopian-io/@scipio/learn-python-series-9-using-import)
- [Learn Python Series (#10) - Matplotlib Part 1](https://steemit.com/utopian-io/@scipio/learn-python-series-10-matplotlib-part-1)
- [Learn Python Series (#11) - NumPy Part 1](https://steemit.com/utopian-io/@scipio/learn-python-series-11-numpy-part-1)
- [Learn Python Series (#12) - Handling Files](https://steemit.com/utopian-io/@scipio/learn-python-series-12-handling-files)
- [Learn Python Series (#13) - Mini Project - Developing a Web Crawler Part 1](https://steemit.com/utopian-io/@scipio/learn-python-series-13-mini-project-developing-a-web-crawler-part-1)
- [Learn Python Series (#14) - Mini Project - Developing a Web Crawler Part 2](https://steemit.com/utopian-io/@scipio/learn-python-series-14-mini-project-developing-a-web-crawler-part-2)
- [Learn Python Series (#15) - Handling JSON](https://steemit.com/utopian-io/@scipio/learn-python-series-15-handling-json)
- [Learn Python Series (#16) - Mini Project - Developing a Web Crawler Part 3](https://steemit.com/utopian-io/@scipio/learn-python-series-16-mini-project-developing-a-web-crawler-part-3)
- [Learn Python Series (#17) - Roundup #2 - Combining and analyzing any-to-any multi-currency historical data](https://steemit.com/utopian-io/@scipio/learn-python-series-17-roundup-2-combining-and-analyzing-any-to-any-multi-currency-historical-data)
- [Learn Python Series (#18) - PyMongo Part 1](https://steemit.com/utopian-io/@scipio/learn-python-series-18-pymongo-part-1)
- [Learn Python Series (#19) - PyMongo Part 2](https://steemit.com/utopian-io/@scipio/learn-python-series-19-pymongo-part-2)
- [Learn Python Series (#20) - PyMongo Part 3](https://steemit.com/utopian-io/@scipio/learn-python-series-20-pymongo-part-3)
- [Learn Python Series (#21) - Handling Dates and Time Part 1](https://steemit.com/utopian-io/@scipio/learn-python-series-21-handling-dates-and-time-part-1)
- [Learn Python Series (#22) - Handling Dates and Time Part 2](https://steemit.com/utopian-io/@scipio/learn-python-series-22-handling-dates-and-time-part-2)
- [Learn Python Series (#23) - Handling Regular Expressions Part 1](https://steemit.com/utopian-io/@scipio/learn-python-series-23-handling-regular-expressions-part-1)
- [Learn Python Series (#24) - Handling Regular Expressions Part 2](https://steemit.com/utopian-io/@scipio/learn-python-series-24-handling-regular-expressions-part-2)
- [Learn Python Series (#25) - Handling Regular Expressions Part 3](https://steemit.com/utopian-io/@scipio/learn-python-series-25-handling-regular-expressions-part-3)
- [Learn Python Series (#26) - pipenv & Visual Studio Code](https://steemit.com/utopian-io/@scipio/learn-python-series-26-pipenv-and-visual-studio-code)
- [Learn Python Series (#27) - Handling Strings Part 3 (F-Strings)](https://steemit.com/utopian-io/@scipio/learn-python-series-27-handling-strings-part-3-f-strings)
- [Learn Python Series (#28) - Using Pickle and Shelve](https://steemit.com/utopian-io/@scipio/learn-python-series-28-using-pickle-and-shelve)
- [Learn Python Series (#29) - Handling CSV](https://steemit.com/utopian-io/@scipio/learn-python-series-29-handling-csv)
- [Learn Python Series (#30) - Data Science Part 1 - Pandas](https://steemit.com/utopian-io/@scipio/learn-python-series-30-data-science-part-1-pandas)

### Additional sample code files
The full - and working! - iPython tutorial sample code file is included for you to download and run for yourself right here:
https://github.com/realScipio/learn-python-series/blob/master/lps-031/learn-python-series-031-data-science-pt2-pandas.ipynb

I've also uploaded to GitHub a CSV file containing **all actual BTCUSDT 1-minute ticks** on Binance on dates June 2, 2019, June 3, 2019 and June 4, 2019 (4320 rows of actual price data, `datetime`, `open`, `high`, `low`, `close` and `volume`), which file and data set we'll be using:
https://github.com/realScipio/learn-python-series/blob/master/lps-031/btcusdt_20190602_20190604_1min_hloc.csv

### GitHub Account
https://github.com/realScipio

# Learn Python Series (#31) - Data Science Part 2 - Pandas
Welcome to episode #31 of the `Learn Python Series`! In the previous episode ([Learn Python Series (#30) - Data Science Part 1 - Pandas](https://steemit.com/utopian-io/@scipio/learn-python-series-30-data-science-part-1-pandas)) I've introduced you to the `pandas` toolkit, and we've already explained some of the basic mechanisms. In this episode (which is no.2 of the Data Science sub-series, also about `pandas`) we'll expand our knowledge using more techniques. Let's dive right in!

# Time series indexing & slicing techniques using `pandas`

### Analysing actual BTCUSDT financial data using `pandas`
First, let's download the file `btcusdt_20190602_20190604_1min_hloc.csv` [found here](https://github.com/realScipio/learn-python-series/blob/master/lps-031/btcusdt_20190602_20190604_1min_hloc.csv) on my GitHub account, and save the file to your current working directory.

Next, let's open the file like so:


```python
import pandas as pd
df = pd.read_csv('btcusdt_20190602_20190604_1min_hloc.csv')
df.head()
```




<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>open</th>
      <th>high</th>
      <th>low</th>
      <th>close</th>
      <th>volume</th>
      <th>datetime</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>8545.10</td>
      <td>8548.55</td>
      <td>8535.98</td>
      <td>8537.67</td>
      <td>17.349543</td>
      <td>2019-06-02 00:00:00+00:00</td>
    </tr>
    <tr>
      <th>1</th>
      <td>8537.53</td>
      <td>8543.49</td>
      <td>8524.00</td>
      <td>8534.66</td>
      <td>31.599922</td>
      <td>2019-06-02 00:01:00+00:00</td>
    </tr>
    <tr>
      <th>2</th>
      <td>8533.64</td>
      <td>8540.13</td>
      <td>8529.98</td>
      <td>8534.97</td>
      <td>7.011458</td>
      <td>2019-06-02 00:02:00+00:00</td>
    </tr>
    <tr>
      <th>3</th>
      <td>8534.97</td>
      <td>8551.76</td>
      <td>8534.00</td>
      <td>8551.76</td>
      <td>5.992965</td>
      <td>2019-06-02 00:03:00+00:00</td>
    </tr>
    <tr>
      <th>4</th>
      <td>8551.76</td>
      <td>8554.76</td>
      <td>8544.62</td>
      <td>8549.30</td>
      <td>15.771411</td>
      <td>2019-06-02 00:04:00+00:00</td>
    </tr>
  </tbody>
</table>
</div>




```python
df.shape
```




    (4320, 6)



A quick visual inspection of this CSV file (using `.head()`, and `.shape`) shows that we're dealing with a data set consisting of 4320 data rows, and 6 data columns, being `datetime`,	`open`, `high`, `low`, `close`, and `volume`. 

**Nota bene:** This data set contains **actual trading data** of the BTC_USDT trading pair,  downloaded from the Binance API, sliced to only contain all 1 minute k-lines / candles on (the example) dates 2019-06-02, 2019-06-03, and 2019-06-04, which I then pre-processed especially for this tutorial episode. The original data returned by the Binance API contains timestamp values in the form of "epoch millis" (milliseconds passed since Jan 1st 1970), and I've converted them into valid ISO-8601 timestamps, which can be easily parsed by the `pandas` package, as we'll learn in this tutorial.

Using the Jupyter Notebook / iPython `%matplotlib inline` Magic operation, let's take a look how the spot price of Bitcoin has developed minute by minute on these dates, by plotting the df['open'] price values:


```python
%matplotlib inline
df['open'].plot()
```




    <matplotlib.axes._subplots.AxesSubplot at 0x1211f5f28>



### Datetime conversion & indexing (using `.to_datetime()` and `.set_index()` methods)
The `pandas` library, while converting the CSV data to a DataFrame object, by default added numerical indexes.

The visual plot example showing the open prices per minute just now, contains X-asis values coming from the numerical indexes that `pandas` set for now. Although it is clear we're in fact plotting 4320 opening price values, those numbers don't provide any usable context on **_when_** the price of Bitcoin was developing over time.

Because we're working with time series now, it would be convenient to be able to set and use the timestamps in the `"datetime"` column as index values, and to be able to plot those timestamps for visual reference as well.
But if we inspect the data type of the first timestamp (`2019-06-02 00:00:00+00:00`), by selecting the "datetime" column and the first row (index value 0), like so, we discover that value is now of the string data type:


```python
type(df['datetime'][0])
```




    str



We can convert all "datetime" column values (in one go, with a vectorised column operation) from string objects to `pandas` Timestamp objects, using the `pandas` method `.to_datetime()`:


```python
df['datetime'] = pd.to_datetime(df['datetime'])
type(df['datetime'][0])
```




    pandas._libs.tslibs.timestamps.Timestamp



Next, we can re-index the entire DataFrame to not use the default numerical index values, but the converted Timstamp values instead. The `.set_index()` method is used for this, and calling it will not only use the Timestamps as index values, therewith unlocking a plethora of functionalities, but `.set_index()` will also remove the 'datetime' column from the data set:


```python
df = df.set_index('datetime')
df.head()
```




<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>open</th>
      <th>high</th>
      <th>low</th>
      <th>close</th>
      <th>volume</th>
    </tr>
    <tr>
      <th>datetime</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>2019-06-02 00:00:00+00:00</th>
      <td>8545.10</td>
      <td>8548.55</td>
      <td>8535.98</td>
      <td>8537.67</td>
      <td>17.349543</td>
    </tr>
    <tr>
      <th>2019-06-02 00:01:00+00:00</th>
      <td>8537.53</td>
      <td>8543.49</td>
      <td>8524.00</td>
      <td>8534.66</td>
      <td>31.599922</td>
    </tr>
    <tr>
      <th>2019-06-02 00:02:00+00:00</th>
      <td>8533.64</td>
      <td>8540.13</td>
      <td>8529.98</td>
      <td>8534.97</td>
      <td>7.011458</td>
    </tr>
    <tr>
      <th>2019-06-02 00:03:00+00:00</th>
      <td>8534.97</td>
      <td>8551.76</td>
      <td>8534.00</td>
      <td>8551.76</td>
      <td>5.992965</td>
    </tr>
    <tr>
      <th>2019-06-02 00:04:00+00:00</th>
      <td>8551.76</td>
      <td>8554.76</td>
      <td>8544.62</td>
      <td>8549.30</td>
      <td>15.771411</td>
    </tr>
  </tbody>
</table>
</div>



### Datetime conversion & indexing (using `.read_csv()` arguments `parse_dates=` and `index_col=`)
While reading in the original CSV values from disk, we could have also immediately passed two additional arguments to the `.read_csv()` method (being: `parse_dates=` and `index_col=`), which would have led to the same DataFrame result as we have now:


```python
import pandas as pd
df = pd.read_csv('btcusdt_20190602_20190604_1min_hloc.csv', 
                 parse_dates=['datetime'], index_col='datetime')
df.head()
```




<div>
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>open</th>
      <th>high</th>
      <th>low</th>
      <th>close</th>
      <th>volume</th>
    </tr>
    <tr>
      <th>datetime</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>2019-06-02 00:00:00+00:00</th>
      <td>8545.10</td>
      <td>8548.55</td>
      <td>8535.98</td>
      <td>8537.67</td>
      <td>17.349543</td>
    </tr>
    <tr>
      <th>2019-06-02 00:01:00+00:00</th>
      <td>8537.53</td>
      <td>8543.49</td>
      <td>8524.00</td>
      <td>8534.66</td>
      <td>31.599922</td>
    </tr>
    <tr>
      <th>2019-06-02 00:02:00+00:00</th>
      <td>8533.64</td>
      <td>8540.13</td>
      <td>8529.98</td>
      <td>8534.97</td>
      <td>7.011458</td>
    </tr>
    <tr>
      <th>2019-06-02 00:03:00+00:00</th>
      <td>8534.97</td>
      <td>8551.76</td>
      <td>8534.00</td>
      <td>8551.76</td>
      <td>5.992965</td>
    </tr>
    <tr>
      <th>2019-06-02 00:04:00+00:00</th>
      <td>8551.76</td>
      <td>8554.76</td>
      <td>8544.62</td>
      <td>8549.30</td>
      <td>15.771411</td>
    </tr>
  </tbody>
</table>
</div>



If we again plot these values, now using the DatetimeIndex, our datetime context is plotted on the X-axis as well: nice!


```python
%matplotlib inline
df['open'].plot()
```

    /anaconda3/lib/python3.7/site-packages/pandas/core/arrays/datetimes.py:1172: UserWarning: Converting to PeriodArray/Index representation will drop timezone information.
      "will drop timezone information.", UserWarning)





    <matplotlib.axes._subplots.AxesSubplot at 0x1211da748>






### Date slicing
Now that we've successfully set the 'datetime' column as the DataFrame index, we can also slice that index using date strings! If we want to only use a data subset containing 1 day of trading data (= 1440 K-line ticks in this data set), for example on 2019-06-02, then we simply pass the date string as an argument, like so:


```python
df_20190602 = df['2019-06-02']
df_20190602.tail()
```




<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>open</th>
      <th>high</th>
      <th>low</th>
      <th>close</th>
      <th>volume</th>
    </tr>
    <tr>
      <th>datetime</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>2019-06-02 23:55:00+00:00</th>
      <td>8725.31</td>
      <td>8728.61</td>
      <td>8721.67</td>
      <td>8725.43</td>
      <td>10.443800</td>
    </tr>
    <tr>
      <th>2019-06-02 23:56:00+00:00</th>
      <td>8725.45</td>
      <td>8729.62</td>
      <td>8720.73</td>
      <td>8728.66</td>
      <td>10.184273</td>
    </tr>
    <tr>
      <th>2019-06-02 23:57:00+00:00</th>
      <td>8728.49</td>
      <td>8729.86</td>
      <td>8724.37</td>
      <td>8729.10</td>
      <td>10.440185</td>
    </tr>
    <tr>
      <th>2019-06-02 23:58:00+00:00</th>
      <td>8729.05</td>
      <td>8731.14</td>
      <td>8723.86</td>
      <td>8723.86</td>
      <td>9.132625</td>
    </tr>
    <tr>
      <th>2019-06-02 23:59:00+00:00</th>
      <td>8723.86</td>
      <td>8726.00</td>
      <td>8718.00</td>
      <td>8725.98</td>
      <td>9.637084</td>
    </tr>
  </tbody>
</table>
</div>



As you can see on the `.tail()`output, the last DataFrame row of the newly created `df_20190602` DataFrame is `2019-06-02 23:59:00`, and the `df_20190602` DataFrame only contains 1440 rows of data in total:


```python
df_20190602.shape
```




    (1440, 5)



In case we want to create a DataFrame containing a multiple day window subset, we also add a stop date to the date string slice, like so:


```python
df_20190602_20190603 = df['2019-06-02':'2019-06-03']
df_20190602_20190603.tail()
```




<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>open</th>
      <th>high</th>
      <th>low</th>
      <th>close</th>
      <th>volume</th>
    </tr>
    <tr>
      <th>datetime</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>2019-06-03 23:55:00+00:00</th>
      <td>8170.31</td>
      <td>8178.48</td>
      <td>8162.61</td>
      <td>8169.20</td>
      <td>46.913377</td>
    </tr>
    <tr>
      <th>2019-06-03 23:56:00+00:00</th>
      <td>8169.24</td>
      <td>8175.39</td>
      <td>8159.00</td>
      <td>8161.52</td>
      <td>44.748378</td>
    </tr>
    <tr>
      <th>2019-06-03 23:57:00+00:00</th>
      <td>8161.16</td>
      <td>8161.17</td>
      <td>8135.94</td>
      <td>8146.69</td>
      <td>65.794209</td>
    </tr>
    <tr>
      <th>2019-06-03 23:58:00+00:00</th>
      <td>8146.58</td>
      <td>8153.00</td>
      <td>8137.38</td>
      <td>8140.00</td>
      <td>43.648000</td>
    </tr>
    <tr>
      <th>2019-06-03 23:59:00+00:00</th>
      <td>8140.00</td>
      <td>8142.80</td>
      <td>8100.50</td>
      <td>8115.82</td>
      <td>185.607380</td>
    </tr>
  </tbody>
</table>
</div>




```python
df_20190602_20190603.shape
```




    (2880, 5)



**Nota bene:** It's important to note that - unlike on "regular" (index) slicing, - when using date string slicing in `pandas` both lower and upper boundaries are **inclusive**; the data being sliced includes all 1440 data rows from June 3, 2019.

### Time slicing using `.between_time()`
What if we want to slice all 1-minute candles which happened one a specific day (e.g. on 2019-06-02) between a specific time interval, for example between 14:00 UTC and 15:00 UTC? This we can accomplish using the `.between_time()` method.

As arguments, we pass the arguments `start_time`, `end_time`, and we specify `include_end=False` so that the end_time is non-inclusive (if we wouldn't, our new sub-set would include 61 instead of 60 1-minute ticks).


```python
df_20190602_1h = df_20190602.between_time('14:00', '15:00', include_end=False)
df_20190602_1h.tail()
```




<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>open</th>
      <th>high</th>
      <th>low</th>
      <th>close</th>
      <th>volume</th>
    </tr>
    <tr>
      <th>datetime</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>2019-06-02 14:55:00+00:00</th>
      <td>8665.71</td>
      <td>8669.50</td>
      <td>8663.66</td>
      <td>8667.24</td>
      <td>11.401655</td>
    </tr>
    <tr>
      <th>2019-06-02 14:56:00+00:00</th>
      <td>8668.06</td>
      <td>8670.90</td>
      <td>8664.87</td>
      <td>8667.07</td>
      <td>11.358376</td>
    </tr>
    <tr>
      <th>2019-06-02 14:57:00+00:00</th>
      <td>8667.07</td>
      <td>8670.90</td>
      <td>8664.96</td>
      <td>8669.96</td>
      <td>15.915794</td>
    </tr>
    <tr>
      <th>2019-06-02 14:58:00+00:00</th>
      <td>8670.00</td>
      <td>8673.37</td>
      <td>8669.46</td>
      <td>8672.73</td>
      <td>11.153078</td>
    </tr>
    <tr>
      <th>2019-06-02 14:59:00+00:00</th>
      <td>8672.72</td>
      <td>8672.72</td>
      <td>8663.57</td>
      <td>8663.59</td>
      <td>9.850304</td>
    </tr>
  </tbody>
</table>
</div>




```python
df_20190602_1h.shape
```




    (60, 5)



# Some basic `pandas` statistic operations (including gaining some real-life Binance BTCUSDT statistical insights while we're at it)

### `.max()`
To retrieve and return the maximum value of a selected DataFrame column, `pandas` provides the `.max()` method. First we select the column we want to inspect, and run the `.max()` method on that, like so (on the original `df` DataFrame we began with):


```python
df['open'].max()
```




    8808.82



### `.idxmax()`
And in order to return the DatetimeIndex on which the maximum value (within the selected range, or in this case the total dataset) occurred, use `.idxmax()`:


```python
df['open'].idxmax()
```




    Timestamp('2019-06-02 12:48:00+0000', tz='UTC')



Interestingly, if we check `.idxmax()` on the "volume" data, '2019-06-03 23:22' is returned. If we look back at the price plot, we can visually spot a "flash crash" of BTC price happening over the course of a few minutes, in which Bitcoin price dropped from about 8500 to around 7800!

The trading volume of Bitcoin (mostly sells) on Binance happening in just one minute ('2019-06-03 23:22' UTC) was almost 950 Bitcoin!


```python
df['volume'].idxmax()
```




    Timestamp('2019-06-03 23:22:00+0000', tz='UTC')




```python
df['volume'].max()
```




    949.563225



### `.min()`
As you have probably guessed, in order to return the minimum value we use the `.min()` method:


```python
df['open'].min()
```




    7490.2



### `.idxmin()`
Similarly, in order to return the DatetimeIndex on which the minimum value occurred, run `.idxmin()`:


```python
df['open'].idxmin()
```




    Timestamp('2019-06-04 22:00:00+0000', tz='UTC')



### `.mean()`
The mean value of a specific series is found by using the `.mean()` method. Using our entire 3-day dataset, we find that the mean price of Bitcoin between June 2, 2019 and Jun 4, 2019 was 8354.03:


```python
df['open'].mean()
```




    8354.033474537022



Having found already that the maximum amount of Bitcoin traded (within our 3 day range) in just minute was about 950 Bitcoin, let's check the average minute trading volume as well:


```python
df['volume'].mean()
```




    34.18334428472222



### `.sum()`
Suppose we want to compute the total trading volume which happened on our entire 3-day DataFrame, then we can do so easily using the `.sum()` method on the df['volume'] column, like so:


```python
df['volume'].sum()
```




    147672.04731



This number is indeed correct, which we can check by multiplying the amount of 1-minute ticks (4320) in our dataset by the mean 1-minute volume we just returned:


```python
4320 * 34.18334428472222
```




    147672.04730999997



### `.describe()`
The basic statistical values we've been using thus far (which of course can be used on more complex DataFrame operations, which we'll discuss in the forthcoming tutorial episodes), we can also output on (a selection of) the entire DataFrame using the `.describe()` method:


```python
df.describe()
```




<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>open</th>
      <th>high</th>
      <th>low</th>
      <th>close</th>
      <th>volume</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>4320.000000</td>
      <td>4320.000000</td>
      <td>4320.000000</td>
      <td>4320.000000</td>
      <td>4320.000000</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>8354.033475</td>
      <td>8359.921905</td>
      <td>8347.543243</td>
      <td>8353.818926</td>
      <td>34.183344</td>
    </tr>
    <tr>
      <th>std</th>
      <td>358.395024</td>
      <td>357.338897</td>
      <td>359.911089</td>
      <td>358.538551</td>
      <td>54.520356</td>
    </tr>
    <tr>
      <th>min</th>
      <td>7490.200000</td>
      <td>7533.430000</td>
      <td>7481.020000</td>
      <td>7494.110000</td>
      <td>1.351415</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>7985.045000</td>
      <td>7990.270000</td>
      <td>7979.205000</td>
      <td>7984.997500</td>
      <td>11.114809</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>8519.605000</td>
      <td>8524.985000</td>
      <td>8513.490000</td>
      <td>8518.845000</td>
      <td>19.566122</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>8661.080000</td>
      <td>8666.992500</td>
      <td>8656.007500</td>
      <td>8661.200000</td>
      <td>35.273851</td>
    </tr>
    <tr>
      <th>max</th>
      <td>8808.820000</td>
      <td>8814.780000</td>
      <td>8805.850000</td>
      <td>8809.910000</td>
      <td>949.563225</td>
    </tr>
  </tbody>
</table>
</div>



# What did we learn, hopefully?
Hopefully you've learned the difference between regular integer index values and DateTimeIndexes, and why and how those are useful on Time Series analysis using `pandas`.

### Thank you for your time!
